import streamlit as st
import requests
from bs4 import BeautifulSoup
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lex_rank import LexRankSummarizer

st.title("ğŸ“° è¨˜äº‹è¦ç´„ã‚¢ãƒ—ãƒª")

url = st.text_input("ğŸ”— è¨˜äº‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

if st.button("è¦ç´„ã™ã‚‹") and url:
    try:
        # è¨˜äº‹ã‚’å–å¾—
        response = requests.get(url, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        content = "\n".join([p.get_text() for p in paragraphs if len(p.get_text()) > 40])

        st.subheader("ğŸ“„ æŠ½å‡ºã•ã‚ŒãŸæœ¬æ–‡")
        st.write(content[:1000] + "..." if len(content) > 1000 else content)

        parser = PlaintextParser.from_string(content, Tokenizer("english"))  # â† ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ
        summarizer = LexRankSummarizer()
        summary = summarizer(parser.document, sentences_count=5)

        st.subheader("âœ‚ï¸ è¦ç´„ï¼ˆ5æ–‡ï¼‰")
        for sentence in summary:
            st.write("ãƒ»", sentence)

    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š{e}")
