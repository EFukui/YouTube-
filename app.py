import streamlit as st
from newspaper import Article
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lex_rank import LexRankSummarizer

st.set_page_config(page_title="è¨˜äº‹è¦ç´„ã‚¢ãƒ—ãƒª", layout="centered")

st.title("ğŸ“° è¨˜äº‹è¦ç´„ã‚¢ãƒ—ãƒª")
st.markdown("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚„ãƒ–ãƒ­ã‚°ã®è¨˜äº‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚æœ¬æ–‡ã‚’å–å¾—ã—ã¦ã€è‡ªå‹•çš„ã«è¦ç´„ã—ã¾ã™ã€‚")

url = st.text_input("ğŸ”— è¨˜äº‹ã®URLã‚’å…¥åŠ›", placeholder="https://example.com/news")

if st.button("è¦ç´„ã™ã‚‹") and url:
    try:
        # è¨˜äº‹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£æ
        article = Article(url, language="ja")
        article.download()
        article.parse()

        st.subheader("ğŸ“ è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«")
        st.write(article.title)

        # æœ¬æ–‡ã‚’è¡¨ç¤ºï¼ˆä»»æ„ï¼‰
        with st.expander("ğŸ“„ è¨˜äº‹ã®å…¨æ–‡ã‚’è¡¨ç¤º"):
            st.write(article.text)

        # è¦ç´„ã‚’ç”Ÿæˆ
        parser = PlaintextParser.from_string(article.text, Tokenizer("japanese"))
        summarizer = LexRankSummarizer()
        summary = summarizer(parser.document, sentences_count=5)

        st.subheader("âœ‚ï¸ è¦ç´„ï¼ˆ5æ–‡ï¼‰")
        for sentence in summary:
            st.write("ãƒ»", sentence)

    except Exception as e:
        st.error(f"è¨˜äº‹ã®å–å¾—ã¾ãŸã¯è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")
