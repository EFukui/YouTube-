import streamlit as st
import requests
from bs4 import BeautifulSoup
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lex_rank import LexRankSummarizer

st.set_page_config(page_title="è¨˜äº‹è¦ç´„ã‚¢ãƒ—ãƒª", layout="centered")

st.title("ğŸ“° è¨˜äº‹è¦ç´„ã‚¢ãƒ—ãƒª")
st.markdown("è¨˜äº‹URLã‚’å…¥åŠ›ã™ã‚‹ã¨ã€æœ¬æ–‡ã‚’å–å¾—ã—ã¦è¦ç´„ã—ã¾ã™ã€‚")

url = st.text_input("ğŸ”— è¨˜äº‹ã®URLã‚’å…¥åŠ›", placeholder="https://example.com/news")

if st.button("è¦ç´„ã™ã‚‹") and url:
    try:
        # HTMLå–å¾—ã¨ãƒ‘ãƒ¼ã‚¹
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")

        # ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚‰ã—ãæ®µè½å–å¾—
        title = soup.title.string if soup.title else "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜ï¼‰"
        paragraphs = soup.find_all("p")
        content = "\n".join([p.get_text() for p in paragraphs if len(p.get_text()) > 40])

        st.subheader("ğŸ“ è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«")
        st.write(title)

        with st.expander("ğŸ“„ æœ¬æ–‡ã‚’è¡¨ç¤º"):
            st.write(content)

        # è¦ç´„
        parser = PlaintextParser.from_string(content, Tokenizer("japanese"))
        summarizer = LexRankSummarizer()
        summary = summarizer(parser.document, sentences_count=5)

        st.subheader("âœ‚ï¸ è¦ç´„ï¼ˆ5æ–‡ï¼‰")
        for sentence in summary:
            st.write("ãƒ»", sentence)

    except Exception as e:
        st.error(f"å–å¾—ã¾ãŸã¯è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")
